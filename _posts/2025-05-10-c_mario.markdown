---
layout: post
title: "Generating Playable Mario Levels from Agent Experience"
thumbnail: /assets/images/mario.png
date: "2025-05-10"
show_header: false
---

<h2>
Generating Playable Mario Levels from Agent Experience
</h2>


This project explores the intersection of procedural content generation (PCG) and reinforcement learning (RL) for creating playable Super Mario Bros levels. By combining generative models with trained RL agents, we enable the automatic design of levels with controllable difficulty.

<h3>Abstract</h3>

We propose a dual pipeline framework where generative models (DCGAN and diffusion models) are trained to generate Mario levels conditioned on empirically derived difficulty scores. These scores are computed from RL agents (Dueling DQN and PPO) trained to play Mario levels.

- Levels are processed into symbolic representations.
- Generators output new level segments conditioned on difficulty.
- RL agents evaluate playability, enabling iterative feedback.

Key findings:

- DCGANs produced the most coherent and playable levels.
- Diffusion models underperformed, struggling with structural coherence.
- PPO agents achieved stable performance and successfully navigated generated levels.

This approach demonstrates a scalable method for difficulty-aware procedural content generation.

---

<h3>Introduction</h3>

- Generative AI (GenAI) creates new content by learning patterns in data (e.g., GANs, diffusion models).
- Reinforcement Learning (RL) trains agents to learn decision-making by interacting with environments.

Both have been applied to video games, which offer controlled environments for AI research. Prior work (Volz et al. 2018) used DCGANs + evolutionary algorithms to evolve Mario levels.

Our contribution: Replace scripted evolutionary search with learning agents (DQN, PPO) to evaluate difficulty, and integrate conditional GANs & diffusion models for controllable level generation.

---

<h3>Methodology</h3>
The pipeline consists of two main stages:

- Agent Training & Difficulty Scoring
    - RL agents play training levels until proficient.
    - A quality score is computed from completion, retries, movements, and rewards.
    - Levels are labeled as Easy, Medium, Hard.
- Generative Models for Level Design
    - Models trained on symbolic Mario levels.
    - Conditioned on difficulty labels.
    - Output new playable levels.
    
<img src="/assets/images/mario_pipeline.png"
     alt="mario_pipeline"
     class="post-image"
     style="width: 600px; display: block;">

---

Levels Processing
- Levels are stored as symbolic text (characters → tiles).
- Converted into identity matrices → one-hot/dense embeddings.
- Models trained on abstract representations, decoded back into symbolic → PNGs.

<img src="/assets/images/mario_levelprocessing.png"
     alt="mario_pipeline"
     class="post-image"
     style="width: 600px; display: block; align:left;">

---
<h3>Level Generation Models</h3>

<h4>Baseline MLP</h4>

- Simple fully-connected generator.
- Fails to preserve spatial structure.
- Output: unplayable, mostly floor tiles.

<img src="/assets/images/mario_baseline.png"
     alt="belief_distorted"
     class="post-image"
     style="width: 600px; display: block;">

<h4>DCGAN</h4>
- Uses transposed convolutions for spatial preservation.
- Trained with adversarial setup.
- Required hyperparameter search (learning rates, batch size, latent dimension).
- Generated coherent Mario-like levels.

<img src="/assets/images/mario_levels.png"
     alt="belief_distorted"
     class="post-image"
     style="width: 600px; display: block;">

---

<h3>RL Agents</h3>
Dueling DQN
- Splits Q-value into Value stream and Advantage stream.
- Improves stability and sample efficiency.

PPO (Proximal Policy Optimization)
- Actor-Critic with clipped surrogate objective.
- Stable and efficient for sparse reward settings.
- Outperformed DQN in final evaluations.
